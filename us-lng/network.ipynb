{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=\"datasets/\"\n",
    "folder=\"energy/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(folder + \"cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"post_type\"].isna()]\n",
    "df=df[~df[\"created_at\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df[\"post_type\"]==\"retweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "words1 = [\"Baker Hughes\",\"BKR.O\",\"Venture Global\",\"VG.N\",\"Plaquemines\",\"Calcasieu Pass\",\"EXXON\",\"$XOM\",\"Repsol\",\"$REPYY\",\"Hanwha\",\"$KRW\",\"Argent LNG\",\"Woodside Energy\",\"$WDS\",\"Cheniere\",\"ConocoPhillips\",\"$COP\",\"Petronas\",\"PNAGF\"]\n",
    "words2 = [\"gas\",\"plant\",\"drill\"]\n",
    "\n",
    "def fast_match(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text1 = text.lower()\n",
    "    for word1 in words1:\n",
    "        if word1.lower() in text1:\n",
    "            if re.search(r'\\b' + re.escape(word1.lower()) + r'\\b', text1):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "camp1 = df[df['content'].apply(fast_match)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "words1 = [\"BP.L\",\"SHEL.L\",\"Edison SpA\",\"EDNn.MI\",\"TotalEnergies\",\"$TTE\",\"Equinor\",\"EQNR\",\"Chevron\",\"CVX\",\"ADNOC\",\"SOCAR\",\"KRX\",\"QatarEnergy\",\"QFLS\",\"CNOOC\",\"PetroChina\",\"Gasprom\",\"Novatek\"]\n",
    "words2 = [\"gas\",\"plant\",\"drill\",\"NewMed\"]\n",
    "\n",
    "def fast_match(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text1 = text.lower()\n",
    "    for word1 in words1:\n",
    "        if word1.lower() in text1:\n",
    "            if re.search(r'\\b' + re.escape(word1.lower()) + r'\\b', text1):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "camp2_1 = df[df['content'].apply(fast_match)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "words1 = [\"BP\",\"Shell\"]\n",
    "words2 =[\"gas\",\"plant\",\"drill\",\"NewMed\"]\n",
    "\n",
    "def fast_match(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text1 = text.lower()\n",
    "    for word1 in words1:\n",
    "        if word1.lower() in text1:\n",
    "            if re.search(r'\\b' + re.escape(word1.lower()) + r'\\b', text1):\n",
    "                for word2 in words2:\n",
    "                    if re.search(r'\\b' + re.escape(word2.lower()) + r'\\b', text1):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "camp2_2 = df[df['content'].apply(fast_match)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camp2=pd.concat([camp2_1, camp2_2], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "camp2=camp2.drop_duplicates(subset='item_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "words1 = [\"UAE\",\"QATAR\",\"Saudi Arabia\",\"Russia\",\"China\",\"EU\",\"Europe\",\"Egypt\",\"Israel\",\"America\"]\n",
    "words2 = [\"gas\"]\n",
    "\n",
    "def fast_match(text):\n",
    "    if pd.isna(text):\n",
    "        return False\n",
    "    text1 = text.lower()\n",
    "    for word1 in words1:\n",
    "        if word1.lower() in text1:\n",
    "            if re.search(r'\\b' + re.escape(word1.lower()) + r'\\b', text1):\n",
    "                for word2 in words2:\n",
    "                    if re.search(r'\\b' + re.escape(word2.lower()) + r'\\b', text1):\n",
    "                        return True\n",
    "    return False\n",
    "\n",
    "camp3 = df[df['content'].apply(fast_match)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng_salient=pd.concat([camp1, camp2,camp3], ignore_index=True) \n",
    "lng_salient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng_salient=lng_salient.drop_duplicates(subset='item_id', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_users=lng_salient[\"user_id\"].to_list()+lng_salient[\"parent_user_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_tweets=lng_salient[\"item_id\"].to_list()+lng_salient[\"parent_item_id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df = pd.DataFrame({'Id': list(set(energy_tweets))})\n",
    "energy_df.to_csv(folder+\"graph/lng_salient_tweet_ids.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng_salient.to_csv(folder+\"graph/lng_salient_retweets.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lng_salient=pd.read_csv(folder+\"graph/lng_salient_retweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_df = pd.DataFrame({'Id': list(set(energy_users)), 'type': 'energy'})\n",
    "energy_df.to_csv(folder+\"graph/energy_users.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df=pd.read_csv(folder + \"cleaned_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "print(df.shape)\n",
    "df = df[df['post_type'] == 'retweet']\n",
    "print(df.shape)\n",
    "df = df.dropna(subset=['user_id', 'parent_user_id'])\n",
    "print(df.shape)\n",
    "\n",
    "df = df[(df['user_id'] != '') & (df['parent_user_id'] != '')]\n",
    "print(df.shape)\n",
    "df = df[(df['user_id'].notna()) & (df['parent_user_id'].notna())]\n",
    "print(df.shape)\n",
    "\n",
    "a=set()\n",
    "graph = nx.Graph()  \n",
    "for _, row in df.iterrows():\n",
    "    user_id = row['user_id']\n",
    "    a.add(user_id)\n",
    "    parent_user_id = row['parent_user_id']\n",
    "    a.add(parent_user_id)\n",
    "    if graph.has_edge(user_id, parent_user_id):\n",
    "        graph[user_id][parent_user_id]['weight'] += 1\n",
    "    elif graph.has_edge(parent_user_id, user_id):\n",
    "        graph[parent_user_id][user_id]['weight'] += 1\n",
    "    else:\n",
    "        graph.add_edge(user_id, parent_user_id, weight=1)\n",
    "#edges_to_remove = [(u, v) for u, v, d in graph.edges(data=True) if d['weight'] < edge_threshold]   \n",
    "#graph.remove_edges_from(edges_to_remove)\n",
    "edge_list = [(u, v, d['weight']) for u, v, d in graph.edges(data=True)]\n",
    "edges_df = pd.DataFrame(edge_list, columns=['source', 'target', 'weight'])\n",
    "edges_df.to_csv(folder+\"graph/graph_all.csv\", index=False)\n",
    "nx.write_gexf(graph,folder+\"graph/all.gexf\")\n",
    "print(len(a))\n",
    "print(f\"Total unique users: {len(a)}\")\n",
    "print(f\"Graph edges saved to graph_all.csv with {len(edge_list)} edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "energy_users=folder+\"graph/energy_users.csv\"\n",
    "energy_users=pd.read_csv(energy_users)\n",
    "energy_users=energy_users[\"Id\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df=pd.read_csv(folder+\"graph/energy_users.csv\")\n",
    "user_list=users_df[\"Id\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from community import community_louvain\n",
    "\n",
    "def louvain_algorithm(graph_G: nx.Graph, resolution: float) -> pd.DataFrame:\n",
    "    partition = community_louvain.best_partition(graph_G, weight='weight', resolution=resolution, random_state=42)\n",
    "    return pd.DataFrame(partition.items(), columns=['node_id', 'community'])\n",
    "\n",
    "def compute_common_users(user_community_df: pd.DataFrame, user_list: list):\n",
    "    community_stats = user_community_df.groupby('community')['node_id'].apply(list).reset_index()\n",
    "    community_stats['common_user_count'] = community_stats['node_id'].apply(lambda x: len(set(x) & set(user_list)))\n",
    "    mean_plus_sigma = community_stats['common_user_count'].mean() + community_stats['common_user_count'].std()\n",
    "    print(mean_plus_sigma)\n",
    "    filtered_communities = community_stats[community_stats['common_user_count'] > mean_plus_sigma]['community']\n",
    "    return user_community_df[user_community_df['community'].isin(filtered_communities)]\n",
    "\n",
    "def export_filtered_graph(G: nx.Graph, user_community_df: pd.DataFrame, nodes_csv: str, edges_csv: str):\n",
    "    subgraph_nodes = set(user_community_df['node_id'])\n",
    "    subgraph = G.subgraph(subgraph_nodes).copy()\n",
    "    nodes_df = pd.DataFrame(list(subgraph.nodes(data=True)), columns=['node_id', 'attributes'])\n",
    "    nodes_df['community'] = nodes_df['node_id'].map(user_community_df.set_index('node_id')['community'])\n",
    "    nodes_df = pd.concat([nodes_df.drop(['attributes'], axis=1), nodes_df['attributes'].apply(pd.Series)], axis=1)\n",
    "    edges_df = pd.DataFrame(list(subgraph.edges(data=True)), columns=['source', 'target', 'attributes'])\n",
    "    edges_df = pd.concat([edges_df.drop(['attributes'], axis=1), edges_df['attributes'].apply(pd.Series)], axis=1)\n",
    "    nodes_df.to_csv(nodes_csv, index=False)\n",
    "    edges_df.to_csv(edges_csv, index=False)\n",
    "\n",
    "def driver(input_file: str, user_list: list, nodes_csv: str, edges_csv: str):\n",
    "    G = nx.read_gexf(input_file)\n",
    "    user_community_df = louvain_algorithm(G, resolution=1.0)\n",
    "    filtered_user_community_df = compute_common_users(user_community_df, user_list)\n",
    "    export_filtered_graph(G, filtered_user_community_df, nodes_csv, edges_csv)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file=folder+\"graph/all.gexf\"\n",
    "    nodes_csv=folder+\"graph/lv1_filtered_nodes.csv\"\n",
    "    edges_csv=folder+\"graph/lv1_filtered_edges.csv\"\n",
    "    driver(input_file, user_list, nodes_csv, edges_csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
